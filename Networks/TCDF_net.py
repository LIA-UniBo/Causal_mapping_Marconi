import torch
import torch.nn as nn
import torch.nn.functional as functional
from torch.nn.functional import softmax


class FirstLayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding):
        super(FirstLayer, self).__init__()
        self.padding = padding
        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation, groups=out_channels)
        self.prelu = nn.PReLU(in_channels)


    def forward(self, x):
        out = self.conv1d(x)[:, :, :-self.padding]     #delete the last elements generated by the padding
        out = self.prelu(out)
        return out

class MiddleLayer(nn.Module):
    '''
    Middle layers of the nn, residual added
    '''
    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding):
        super(MiddleLayer, self).__init__()
        self.padding = padding
        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation, groups=out_channels)
        self.relu = nn.PReLU(in_channels)

    def forward(self, x):
        out = self.conv1d(x)[:, :, :-self.padding]  # delete the last elements generated by the padding
        out = self.relu(out + x)  # residual
        return out



class FinalLayer(nn.Module):
    '''
    Final layers of the nn, linear layer instead of prelu and residual added
    '''
    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding):
        super(FinalLayer, self).__init__()
        self.padding = padding
        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation, groups=out_channels)
        self.linear = nn.Linear(in_channels, in_channels)


    def forward(self, x):
        out = self.conv1d(x)[:, :, :-self.padding]     #delete the last elements generated by the padding
        out = self.linear((out + x).transpose(1,2)).transpose(1,2)   #residual and linear layer
        return out


class ADDSTCN(nn.Module):
    def __init__(self, in_channels, levels, kernel_size, dilation, device):
        super(ADDSTCN, self).__init__()
        self.device = device
        dilation_first_layer = 1
        self.first_layer = FirstLayer(in_channels, in_channels, kernel_size, dilation=dilation_first_layer, padding=(kernel_size - 1) * dilation_first_layer)
        dilation_final_layer = dilation**(levels-1)
        self.final_layer = FinalLayer(in_channels, in_channels, kernel_size, dilation=dilation_final_layer, padding=(kernel_size - 1) * dilation_final_layer)
        self.final_conv  = nn.Conv1d(in_channels, 1, 1)
        self.middle_layers = nn.Sequential()
        for level in range(levels-2):
            dilation_middle_layer =  dilation_final_layer = dilation**(level+1)
            self.middle_layers.add_module("Middle layer "+str(level),MiddleLayer(in_channels, in_channels, kernel_size,
                                                      dilation=dilation_middle_layer, padding=(kernel_size - 1) * dilation_middle_layer))

        self.attention_param =  torch.autograd.Variable(torch.ones(in_channels, 1), requires_grad=False)
        self.attention = torch.nn.Parameter(self.attention_param.data)



    def forward(self, x):
        x = x.to(self.device)
        attention_softmax = functional.softmax(self.attention, dim=0)
        out_soft = x*attention_softmax
        out_first = self.first_layer(out_soft)
        out_middle = self.middle_layers(out_first)
        out_final = self.final_layer(out_middle)
        out = self.final_conv(out_final)
        return out


